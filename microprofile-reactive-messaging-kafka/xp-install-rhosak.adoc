// Keeping this file in the reactive messaging quickstart for now (rather than ../shared-doc
// since it will be quite application specific
[[install_rhosak]]
= Install RHOSAK on Openshift

The functionality of this quickstart depends on a running instance of the
https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka[Red Hat OpenShift Streams for Apache Kafka (RHOSAK)] Operator. RHOSAK is a Red Hat managed cloud service based on Apache Kafka. It runs on OpenShift but outside your OpenShift environment.

== Setting up RHOSAK
We will summarize the steps required to set up a Kafka instance in RHOSAK here. They were correct at the time of writing and inspired by the following https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance[article] explaining how to set up an instance in the Developer Sandbox for Red Hat OpenShift, which allows you to try technologies on OpenShift for free. If these steps change, the RHOSAK documentation will have the up to date instructions. Finally, the steps below assume you are using the Developer Sandbox. If you are a paying customer of RHOSAK you should adjust the steps accordingly.

*Prerequisites:*

* On your OpenShift cluster make sure that the `RHOAS` and `Service Binding Operator` operators are installed.
** You can check this by going to Operators/Installed Operators in the Administrator perspective of the console
** If not installed, install (or ask an admin if you don't have permissions) these operators via Operators/OperatorHub in the Administrator perspective of the console
* Create a https://developers.redhat.com/products/rhosak/getting-started[RHOSAK account] before doing the following steps:

1. From the https://developers.redhat.com/products/rhosak/getting-started[RHOSAK] console, create a Kafka instance. You need to specify a name for it Kafka instance, for example `my-quickstart-kafka`. In the rest of this text we will use `<kafka-name>` to represent `my-quickstart-kafka`. Apart from the name you can use default values for everything else. It will take a few minutes for your Kafka instance to be ready.
2. Go into the instance and create a topic called `testing`. Use the suggested defaults for everything else.
3. https://github.com/redhat-developer/app-services-cli[Download] the `rhoas` application for your OS. Make it available on your path.

## Connecting the application to Kafka

First we need to log in to RHOAS. The login process uses the browser session for the console.
----
rhoas login
----
Then we need to make `rhoas` use our Kafka instance
----
# Substitute <kafka-name> with the name of your Kafka instance
rhoas kafka use --name <kafka-name>
----
Then go to https://console.redhat.com/openshift/token and get the token to authenticate with your OpenShift cluster.

Execute
----
rhoas cluster connect --service-type kafka --yes --token {your token pasted here}
----
The output of the above will show that the connection to Kafka has been created. Make a note of the `Client ID` which appears in the output of the `rhoas cluster connect` command. We will need this `Client ID` later:
----
✔️  Service Account Secret "rh-cloud-services-service-account" created successfully

Client ID:     srvc-acct-4321abcd-123a-98cb-1234-01234abcd123

Make a copy of the client ID to store in a safe place. Credentials won't appear again after closing the terminal.
----
At the time of writing, if you did not make a note of the `Client ID`, it is possible to recover it by running `rhoas service-account list`, and looking for the `Client ID` associated with your account.

The `rhoas cluster connect` output will also tell you to run `rhoas cluster bind`. However, we need to wait until we have deployed our application with `helm install` in the next step. Once we run `rhoas cluster bind --binding-name kafka-config` after  deploying our application, the Service Binding Operator will create a config map bound to the `/bindings/kafka-config` directory in the application pods. We will use this service binding in the next section, although we need to wait before we actually perform this binding.

Next, we need to grant access to our service account. This is done by running the following command
----
rhoas kafka acl grant-access --consumer --producer     --service-account <client id> --topic "*" --group "*"
----
Substitute <client id> with the `Client ID` reported by the earlier `rhoas cluster connect` command.

== Configuring your application
The final part of the setup is to map the created service binding config map to our application. As mentioned, once we run `rhoas cluster bind`, we will get a Config Map with entries mapped to files under the /bindings/kafka-config` directory in the application pods.

When building the application the link:/src/main/scripts/rhosak/s2i/initialize-server.cli[] CLI script gets run, and performs the following steps.

First it adds the config map stored in the `/bindings/kafka-config` directory as a config source in the MicroProfile Config subsystem.
----
echo "Adding the kafka-config service binding..."
/subsystem=microprofile-config-smallrye/config-source=rhosak-binding:add(dir={path=/bindings/kafka-config})/subsystem=microprofile-config-smallrye/config-source=rhosak-binding:add(dir={path=/bindings/kafka-config})
----

This config map contains properties such as `bootstrapServers`, `securityProtocol`, `saslMechanism`, `user` and `password`. These contain all the information we need to connect to the managed Kafka instance. However, we need to map these onto what our application understands, and this is done in the second part of the CLI script:
-----
echo "Adding the MicroProfile Config entries mapping the service binding..."
/subsystem=microprofile-config-smallrye/config-source=reactive-messaging-properties:add(properties={\
mp.messaging.connector.smallrye-kafka.bootstrap.servers=${bootstrapServers},\
mp.messaging.connector.smallrye-kafka.security.protocol=${securityProtocol},\
mp.messaging.connector.smallrye-kafka.sasl.mechanism=${saslMechanism},\
mp.messaging.connector.smallrye-kafka.sasl.jaas.config="\n\
org.apache.kafka.common.security.plain.PlainLoginModule required\n\
username=\"${user}\"\n\
password=\"${password}\";"\
}, ordinal=500)
-----

This has the effect of, for example, the value `mp.messaging.connector.smallrye-kafka.bootstrap.servers` will use the `${bootstrapServers}` value provided by the service binding config map.

These properties from the MicroProfile Config subsystem have the same names as some of the ones we already defined in link:src/main/resources/META-INF/microprofile-config.properties[microprofile-config.properties]. These get merged into one overall configuration. Where the same value exists in both places the one added in the CLI script above takes precedence, so `mp.messaging.connector.smallrye-kafka.bootstrap.servers` from the values provided by CLI script will be used. This is because the CLI script uses a higher `ordinal` when adding these properties.
Once deployed this will store the ConfigMap settings in the application pod's `/etc/config/reactive-messaging-properties` directory.

The CLI script will get run as part of initialising the application pod, when we pass in the environment variable `QS_USE_RHOSAK=true` when building the application as we will see later. For the bootable jar with RHOSAK combination there is a `src/main/scripts/rhoasak/bootable-jar/initialise-server.cli` script that does the same.